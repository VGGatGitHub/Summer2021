{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "QAlpha-Knapsack-July.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VGGatGitHub/Summer2021/blob/main/QAlpha_Knapsack_July.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSuiKSaAB7NO",
        "outputId": "5d686c8c-7a42-4603-88fe-437673fc6ec1"
      },
      "source": [
        "!pip install docplex\n",
        "!pip install cplex"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: docplex in /usr/local/lib/python3.7/dist-packages (2.21.207)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from docplex) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from docplex) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->docplex) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->docplex) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->docplex) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->docplex) (2021.5.30)\n",
            "Collecting cplex\n",
            "  Downloading cplex-20.1.0.1-cp37-cp37m-manylinux1_x86_64.whl (30.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 30.9 MB 32 kB/s \n",
            "\u001b[?25hInstalling collected packages: cplex\n",
            "Successfully installed cplex-20.1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbGaZt44-7a0",
        "outputId": "ccfa744f-d4be-4dc6-9d9f-28fd4efbf7ad"
      },
      "source": [
        "from docplex.mp.model import *\n",
        "from docplex.mp.utils import *\n",
        "from docplex.util.status import JobSolveStatus\n",
        "from docplex.mp.conflict_refiner import ConflictRefiner, VarUbConstraintWrapper, VarLbConstraintWrapper\n",
        "from docplex.mp.relaxer import Relaxer\n",
        "import time\n",
        "import sys\n",
        "import operator\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import codecs\n",
        "import sys\n",
        "\n",
        "# Handle output of unicode strings\n",
        "if sys.version_info[0] < 3:\n",
        "    sys.stdout = codecs.getwriter('utf8')(sys.stdout)\n",
        "\n",
        "\n",
        "from pandas.api.types import is_string_dtype\n",
        "\n",
        "\n",
        "#Load all input data as a map { data_name: data_frame }\n",
        "inputs = {'july': pd.read_csv('./july.csv')}\n",
        "# This will hold all outputs as a map { data_name: data_frame }\n",
        "outputs = {}\n",
        "\n",
        "# we use a lock to access ``outputs``. This allows solves() to\n",
        "# be aborted without race condition in data writting\n",
        "import threading\n",
        "output_lock = threading.Lock()\n",
        "\n",
        "\n",
        "def helper_check_data_type(df, column, df_label, check_type):\n",
        "    if not column in df.columns:\n",
        "        print('Column \"%s\" does not exist in table \"%s\"' % (column, df_label))\n",
        "        return False\n",
        "    non_nan_values = df[column][~df[column].isnull()]\n",
        "    if check_type == 'INTEGER':\n",
        "        k = non_nan_values.dtype.kind\n",
        "        if k != 'i':\n",
        "            if k == 'f':\n",
        "                non_integer_values = non_nan_values.values[np.where([not x.is_integer() for x in non_nan_values])]\n",
        "                if len(non_integer_values) > 0:\n",
        "                    print('Column \"%s\" of table \"%s\" contains non-integer value(s) which violates expected type: %s' % (column, df_label, non_integer_values))\n",
        "                    return False\n",
        "            else:\n",
        "                print('Column \"%s\" of table \"%s\" is non-numeric which violates expected type: %s' % (column, df_label, non_nan_values.values))\n",
        "                return False\n",
        "    elif check_type == 'FLOAT' or check_type == 'NUMBER':\n",
        "        non_float_values = non_nan_values.values[np.where([not isinstance(x, (int, float)) for x in non_nan_values])]\n",
        "        k = non_nan_values.dtype.kind\n",
        "        if not k in ['i', 'f']:\n",
        "            print('Column \"%s\" of table \"%s\" contains non-float value(s) which violates expected type: %s' % (column, df_label, non_float_values))\n",
        "            return False\n",
        "    elif check_type == 'BOOLEAN':\n",
        "        non_bool_values = non_nan_values.values[np.where([not isinstance(x, bool) for x in non_nan_values])]\n",
        "        if len(non_bool_values) > 0:\n",
        "            print('Column \"%s\" of table \"%s\" contains non-boolean value(s) which violates expected type: %s' % (column, df_label, non_bool_values))\n",
        "            return False\n",
        "    elif check_type == 'Date' or check_type == 'DateTime':\n",
        "        try:\n",
        "            pd.to_datetime(non_nan_values)\n",
        "        except ValueError as e:\n",
        "            print('Column \"%s\" of table \"%s\" cannot be converted to a DateTime : %s' % (column, df_label, str(e)))\n",
        "            return False\n",
        "    elif check_type == 'Time':\n",
        "        try:\n",
        "            pd.to_timedelta(non_nan_values)\n",
        "        except ValueError as e:\n",
        "            try:\n",
        "                # Try appending ':00' in case seconds are not represented in time\n",
        "                pd.to_timedelta(non_nan_values + ':00')\n",
        "            except ValueError as e:\n",
        "                print('Column \"%s\" of table \"%s\" cannot be converted to a Time : %s' % (column, df_label, str(e)))\n",
        "                return False\n",
        "    elif check_type == 'STRING':\n",
        "        if not is_string_dtype(non_nan_values):\n",
        "            print('Column \"%s\" of table \"%s\" is not of type \"String\"' % (column, df_label))\n",
        "            return False\n",
        "    else:\n",
        "        raise Exception('Invalid check_type: %s' % check_type)\n",
        "    return True\n",
        "\n",
        "\n",
        "def helper_check_foreignKey_values(source_df, source_column, source_df_label, target_df, target_column, target_df_label):\n",
        "    non_nan_values = source_df[source_column][~source_df[source_column].isnull()]\n",
        "    invalid_FK_values = non_nan_values[~non_nan_values.isin(target_df[target_column])].values\n",
        "    if len(invalid_FK_values) > 0:\n",
        "        print('FK Column \"%s\" of table \"%s\" contains values that do not exist in PK column \"%s\" of target table \"%s\": %s' % (source_column, source_df_label, target_column, target_df_label, invalid_FK_values))\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def helper_check_unique_primaryKey_values(df, key_cols, df_label):\n",
        "    df_grp = df.groupby(key_cols).size()\n",
        "    invalid_pk_values = df_grp[df_grp > 1].reset_index()[key_cols].values\n",
        "    if len(invalid_pk_values) > 0:\n",
        "        print('Non-unique values for PK of table \"%s\": %s' % (df_label, invalid_pk_values))\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "# Label constraint\n",
        "def helper_add_labeled_cplex_constraint(mdl, expr, label, context=None, columns=None):\n",
        "    global expr_counter\n",
        "    if isinstance(expr, np.bool_):\n",
        "        expr = expr.item()\n",
        "    if isinstance(expr, bool):\n",
        "        pass  # Adding a trivial constraint: if infeasible, docplex will raise an exception it is added to the model\n",
        "    else:\n",
        "        expr.name = '_L_EXPR_' + str(len(expr_to_info) + 1)\n",
        "        if columns:\n",
        "            ctxt = \", \".join(str(getattr(context, col)) for col in columns)\n",
        "        else:\n",
        "            if context:\n",
        "                ctxt = context.Index if isinstance(context.Index, str) is not None else \", \".join(context.Index)\n",
        "            else:\n",
        "                ctxt = None\n",
        "        expr_to_info[expr.name] = (label, ctxt)\n",
        "    mdl.add(expr)\n",
        "\n",
        "def helper_get_column_name_for_property(property):\n",
        "    return helper_property_id_to_column_names_map.get(property, 'unknown')\n",
        "\n",
        "\n",
        "def helper_get_index_names_for_type(dataframe, type):\n",
        "    if not is_pandas_dataframe(dataframe):\n",
        "        return None\n",
        "    return [name for name in dataframe.index.names if name in helper_concept_id_to_index_names_map.get(type, [])]\n",
        "\n",
        "\n",
        "helper_concept_id_to_index_names_map = {\n",
        "    'cItem': ['id_of_July'],\n",
        "    'july': ['id_of_July']}\n",
        "helper_property_id_to_column_names_map = {\n",
        "    'july.city': 'city',\n",
        "    'july.sick': 'sick',\n",
        "    'july.gdp': 'gdp'}\n",
        "\n",
        "\n",
        "# Data model definition for each table\n",
        "# Data collection: list_of_July ['sick', 'gdp', 'city']\n",
        "\n",
        "# Create a pandas Dataframe for each data table\n",
        "list_of_July = inputs[u'july']\n",
        "list_of_July = list_of_July[[u'sick', u'gdp', u'city']].copy()\n",
        "list_of_July.rename(columns={u'sick': 'sick', u'gdp': 'gdp', u'city': 'city'}, inplace=True)\n",
        "\n",
        "# Perform input data checking against schema configured in Modelling Assistant along with unicity of PK values\n",
        "data_check_result = True\n",
        "# --- Handling data checking for table: july\n",
        "data_check_result &= helper_check_data_type(list_of_July, 'sick', 'july', 'NUMBER')\n",
        "data_check_result &= helper_check_data_type(list_of_July, 'gdp', 'july', 'NUMBER')\n",
        "data_check_result &= helper_check_unique_primaryKey_values(list_of_July, ['city'], 'july')\n",
        "if not data_check_result:\n",
        "    # Stop execution here\n",
        "    raise Exception('Data checking detected errors')\n",
        "\n",
        "# Force column type for non-numeric columns\n",
        "list_of_July = list_of_July.astype({'city': object})\n",
        "\n",
        "# Set index when a primary key is defined\n",
        "list_of_July.set_index('city', inplace=True)\n",
        "list_of_July.sort_index(inplace=True)\n",
        "list_of_July.index.name = 'id_of_July'\n",
        "\n",
        "\n",
        "#VC: extra scalars unable to be defined by IBM's auto AI\n",
        "gdp_remain_ratio = 0.2\n",
        "sick_remain_ratio = 0.1\n",
        "\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    mdl = Model()\n",
        "\n",
        "    # Definition of model variables\n",
        "    list_of_July['selectionVar'] = mdl.binary_var_list(len(list_of_July))\n",
        "\n",
        "\n",
        "    # Definition of model\n",
        "    # Objective cMaximizeGoalSelect#1-\n",
        "    # Combine weighted criteria: \n",
        "    # \tcMaximizeGoalSelect cMaximizeGoalSelect 1.2{\n",
        "    # \tnumericExpr = cSelection__july__ / july / gdp,\n",
        "    # \tscaleFactorExpr = 1,\n",
        "    # \t(static) goalFilter = null} with weight 8.0\n",
        "    # \tcMinimizeGoalSelect cMinimizeGoalSelect 1.2{\n",
        "    # \tnumericExpr = cSelection__july__ / july / sick,\n",
        "    # \tscaleFactorExpr = 1,\n",
        "    # \t(static) goalFilter = null} with weight 8.0\n",
        "    # VC: 1 - selectionVar for modulo 2 effect\n",
        "    list_of_July['conditioned_gdp'] = (list_of_July.selectionVar + gdp_remain_ratio * (1 - list_of_July.selectionVar)) * list_of_July.gdp\n",
        "    agg_July_conditioned_gdp_SG1 = mdl.sum(list_of_July.conditioned_gdp)\n",
        "    list_of_July['conditioned_sick'] = (list_of_July.selectionVar + sick_remain_ratio * (1 - list_of_July.selectionVar)) * list_of_July.sick\n",
        "    agg_July_conditioned_sick_SG2 = mdl.sum(list_of_July.conditioned_sick)\n",
        "    \n",
        "    kpis_expression_list = [\n",
        "        (-1, 128.0, agg_July_conditioned_gdp_SG1, 1, 0, u'total gdp of julies over all selections'),\n",
        "        (1, 128.0, agg_July_conditioned_sick_SG2, 1, 0, u'total sick of julies over all selections')]\n",
        "    custom_code.update_goals_list(kpis_expression_list)\n",
        "    \n",
        "    for _, kpi_weight, kpi_expr, kpi_factor, kpi_offset, kpi_name in kpis_expression_list:\n",
        "        mdl.add_kpi(kpi_expr, publish_name=kpi_name)\n",
        "    \n",
        "    mdl.minimize(sum([kpi_sign * kpi_weight * ((kpi_expr * kpi_factor) - kpi_offset) for kpi_sign, kpi_weight, kpi_expr, kpi_factor, kpi_offset, kpi_name in kpis_expression_list]))\n",
        "    \n",
        "    # [ST_1] Constraint : cGlobalRelationalConstraint#1_cGlobalRelationalConstraint\n",
        "    # total sick of julies over all selections is less than or equal to 500000\n",
        "    # Label: CT_1_total_sick_of_julies_over_all_selections_is_less_than_or_equal_to_500000\n",
        "    list_of_July['conditioned_sick'] = list_of_July.selectionVar * list_of_July.sick\n",
        "    agg_July_conditioned_sick_lhs = mdl.sum(list_of_July.conditioned_sick)\n",
        "    helper_add_labeled_cplex_constraint(mdl, agg_July_conditioned_sick_lhs <= 500000, u'total sick of julies over all selections is less than or equal to 500000')\n",
        "\n",
        "\n",
        "    return mdl\n",
        "\n",
        "\n",
        "def solve_model(mdl):\n",
        "    mdl.parameters.timelimit = 120\n",
        "    # Call to custom code to update parameters value\n",
        "    custom_code.update_solver_params(mdl.parameters)\n",
        "    # Update parameters value according to environment variables definition\n",
        "    cplex_param_env_prefix = 'ma.cplex.'\n",
        "    cplex_params = [name.qualified_name for name in mdl.parameters.generate_params()]\n",
        "    for param in cplex_params:\n",
        "        env_param = cplex_param_env_prefix + param\n",
        "        param_value = get_environment().get_parameter(env_param)\n",
        "        if param_value:\n",
        "            # Updating parameter value\n",
        "            print(\"Updated value for parameter %s = %s\" % (param, param_value))\n",
        "            parameters = mdl.parameters\n",
        "            for p in param.split('.')[1:]:\n",
        "                parameters = parameters.__getattribute__(p)\n",
        "            parameters.set(param_value)\n",
        "\n",
        "    msol = mdl.solve(log_output=True)\n",
        "    if not msol:\n",
        "        print(\"!!! Solve of the model fails\")\n",
        "        if mdl.get_solve_status() == JobSolveStatus.INFEASIBLE_SOLUTION or mdl.get_solve_status() == JobSolveStatus.INFEASIBLE_OR_UNBOUNDED_SOLUTION:\n",
        "            crefiner = ConflictRefiner()\n",
        "            conflicts = crefiner.refine_conflict(model, log_output=True)\n",
        "            export_conflicts(conflicts)\n",
        "            \n",
        "    print('Solve status: %s' % mdl.get_solve_status())\n",
        "    if mdl.get_solve_status() == JobSolveStatus.UNKNOWN:\n",
        "        print('UNKNOWN cause: %s' % mdl.get_solve_details().status)\n",
        "    mdl.report()\n",
        "    return msol\n",
        "\n",
        "\n",
        "expr_to_info = {}\n",
        "\n",
        "\n",
        "def export_conflicts(conflicts):\n",
        "    # Display conflicts in console\n",
        "    print('Conflict set:')\n",
        "    list_of_conflicts = pd.DataFrame(columns=['constraint', 'context', 'detail'])\n",
        "    for conflict, index in zip(conflicts, range(len(conflicts))):\n",
        "        st = conflict.status\n",
        "        ct = conflict.element\n",
        "        label, context = expr_to_info.get(conflict.name, ('N/A', conflict.name))\n",
        "        label_type = type(conflict.element)\n",
        "        if isinstance(conflict.element, VarLbConstraintWrapper) \\\n",
        "                or isinstance(conflict.element, VarUbConstraintWrapper):\n",
        "            label = 'Upper/lower bound conflict for variable: {}'.format(conflict.element._var)\n",
        "            context = 'Decision variable definition'\n",
        "            ct = conflict.element.get_constraint()\n",
        "\n",
        "        # Print conflict information in console\n",
        "        print(\"Conflict involving constraint: %s, \\tfor: %s -> %s\" % (label, context, ct))\n",
        "        list_of_conflicts = list_of_conflicts.append({'constraint': label, 'context': str(context), 'detail': ct},\n",
        "                                                     ignore_index=True)\n",
        "\n",
        "    # Update of the ``outputs`` dict must take the 'Lock' to make this action atomic,\n",
        "    # in case the job is aborted\n",
        "    global output_lock\n",
        "    with output_lock:\n",
        "        outputs['list_of_conflicts'] = list_of_conflicts\n",
        "\n",
        "\n",
        "def export_solution(msol):\n",
        "    start_time = time.time()\n",
        "    mdl = msol.model\n",
        "    list_of_July_solution = pd.DataFrame(index=list_of_July.index)\n",
        "    list_of_July_solution['selectionVar'] = msol.get_values(list_of_July.selectionVar.values)\n",
        "    NotSelectedJulies = pd.DataFrame(index=list_of_July.index)\n",
        "    \n",
        "    # Adding extra columns based on Solution Schema\n",
        "    NotSelectedJulies['july sick'] = list_of_July['sick']\n",
        "    NotSelectedJulies['july gdp'] = list_of_July['gdp']\n",
        "    \n",
        "    NotSelectedJulies['_INTERNAL_selectionVar'] = list_of_July_solution.selectionVar\n",
        "    NotSelectedJulies = NotSelectedJulies[NotSelectedJulies._INTERNAL_selectionVar <= 0.5]\n",
        "    NotSelectedJulies = NotSelectedJulies.drop('_INTERNAL_selectionVar', axis='columns')\n",
        "\n",
        "    SelectedJulies = pd.DataFrame(index=list_of_July.index)\n",
        "    SelectedJulies['selected julies decision'] = list_of_July_solution['selectionVar']\n",
        "    SelectedJulies['july sick'] = list_of_July['sick']\n",
        "    SelectedJulies['july gdp'] = list_of_July['gdp']\n",
        "    \n",
        "    SelectedJulies['_INTERNAL_selectionVar'] = list_of_July_solution.selectionVar\n",
        "    SelectedJulies = SelectedJulies[SelectedJulies._INTERNAL_selectionVar > 0.5]\n",
        "    SelectedJulies = SelectedJulies.drop('_INTERNAL_selectionVar', axis='columns')\n",
        "\n",
        "\n",
        "    # Update of the ``outputs`` dict must take the 'Lock' to make this action atomic,\n",
        "    # in case the job is aborted\n",
        "    global output_lock\n",
        "    with output_lock:\n",
        "        outputs['NotSelectedJulies'] = NotSelectedJulies[['july sick', 'july gdp']].reset_index().rename(columns= {'id_of_July': 'july'})\n",
        "        outputs['SelectedJulies'] = SelectedJulies[['selected julies decision', 'july sick', 'july gdp']].reset_index().rename(columns= {'id_of_July': 'july'})\n",
        "        custom_code.post_process_solution(msol, outputs)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print('solution export done in ' + str(elapsed_time) + ' secs')\n",
        "    return\n",
        "\n",
        "\n",
        "# Import custom code definition if module exists\n",
        "try:\n",
        "    from custom_code import CustomCode\n",
        "    custom_code = CustomCode(globals())\n",
        "except ImportError:\n",
        "    # Create a dummy anonymous object for custom_code\n",
        "    custom_code = type('', (object,), {'preprocess': (lambda *args: None),\n",
        "                                       'update_goals_list': (lambda *args: None),\n",
        "                                       'update_model': (lambda *args: None),\n",
        "                                       'update_solver_params': (lambda *args: None),\n",
        "                                       'post_process_solution': (lambda *args: None)})()\n",
        "\n",
        "\n",
        "from docplex.mp.progress import ProgressListener\n",
        "from docplex.util.environment import add_abort_callback, remove_abort_callback\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "class SolutionListener(ProgressListener):\n",
        "    def __init__(self):\n",
        "        super(SolutionListener, self).__init__()\n",
        "        self._cpx_incumbent_sol = None\n",
        "\n",
        "    def requires_solution(self):\n",
        "        return True\n",
        "\n",
        "    def notify_solution(self, cpx_incumbent_sol):\n",
        "        self._cpx_incumbent_sol = cpx_incumbent_sol\n",
        "\n",
        "\n",
        "def save_and_write_last_solution_callback(sol_listener, outputs):\n",
        "    if sol_listener._cpx_incumbent_sol:\n",
        "        export_solution(sol_listener._cpx_incumbent_sol)\n",
        "    write_all_outputs(outputs)\n",
        "\n",
        "\n",
        "solution_listener = SolutionListener()\n",
        "\n",
        "# Custom pre-process\n",
        "custom_code.preprocess()\n",
        "\n",
        "print('* building wado model')\n",
        "start_time = time.time()\n",
        "model = build_model()\n",
        "\n",
        "# Model customization\n",
        "custom_code.update_model(model)\n",
        "\n",
        "#\n",
        "model.add_progress_listener(solution_listener)\n",
        "env = get_environment()\n",
        "# Remove default abort callbacks\n",
        "for cb in env.abort_callbacks:\n",
        "    remove_abort_callback(cb)\n",
        "# Add new abort callback to store latest found solution, if any\n",
        "add_abort_callback(partial(save_and_write_last_solution_callback, solution_listener, outputs))\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print('model building done in ' + str(elapsed_time) + ' secs')\n",
        "\n",
        "print('* running wado model')\n",
        "start_time = time.time()\n",
        "msol = solve_model(model)\n",
        "elapsed_time = time.time() - start_time\n",
        "print('model solve done in ' + str(elapsed_time) + ' secs')\n",
        "if msol:\n",
        "    export_solution(msol)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* building wado model\n",
            "model building done in 0.028308391571044922 secs\n",
            "* running wado model\n",
            "Version identifier: 20.1.0.0 | 2020-11-11 | 9bedb6d68\n",
            "CPXPARAM_Read_DataCheck                          1\n",
            "CPXPARAM_TimeLimit                               120\n",
            "Legacy callback                                  i\n",
            "Found incumbent of value -3.2541974e+08 after 0.00 sec. (0.00 ticks)\n",
            "Tried aggregator 1 time.\n",
            "Reduced MIP has 1 rows, 50 columns, and 50 nonzeros.\n",
            "Reduced MIP has 50 binaries, 0 generals, 0 SOSs, and 0 indicators.\n",
            "Presolve time = 0.02 sec. (0.07 ticks)\n",
            "Probing time = 0.00 sec. (0.01 ticks)\n",
            "Tried aggregator 1 time.\n",
            "Reduced MIP has 1 rows, 50 columns, and 50 nonzeros.\n",
            "Reduced MIP has 50 binaries, 0 generals, 0 SOSs, and 0 indicators.\n",
            "Presolve time = 0.01 sec. (0.07 ticks)\n",
            "Probing time = 0.00 sec. (0.01 ticks)\n",
            "MIP emphasis: balance optimality and feasibility.\n",
            "MIP search method: dynamic search.\n",
            "Parallel mode: deterministic, using up to 2 threads.\n",
            "Root relaxation solution time = 0.00 sec. (0.02 ticks)\n",
            "\n",
            "        Nodes                                         Cuts/\n",
            "   Node  Left     Objective  IInf  Best Integer    Best Bound    ItCnt     Gap\n",
            "\n",
            "*     0+    0                      -3.25420e+08  -1.57329e+09           383.46%\n",
            "      0     0  -1.43203e+09     1  -3.25420e+08  -1.43203e+09        1  340.06%\n",
            "*     0+    0                      -1.42766e+09  -1.43203e+09             0.31%\n",
            "      0     0  -1.43125e+09     2  -1.42766e+09    MIRcuts: 1        4    0.25%\n",
            "      0     0  -1.42935e+09     1  -1.42766e+09       Cuts: 3        9    0.12%\n",
            "*     0+    0                      -1.42896e+09  -1.42935e+09             0.03%\n",
            "      0     0        cutoff        -1.42896e+09  -1.42896e+09        9    0.00%\n",
            "Elapsed time = 0.08 sec. (0.41 ticks, tree = 0.01 MB, solutions = 3)\n",
            "\n",
            "Mixed integer rounding cuts applied:  1\n",
            "Zero-half cuts applied:  1\n",
            "Gomory fractional cuts applied:  1\n",
            "\n",
            "Root node processing (before b&c):\n",
            "  Real time             =    0.08 sec. (0.41 ticks)\n",
            "Parallel b&c, 2 threads:\n",
            "  Real time             =    0.00 sec. (0.00 ticks)\n",
            "  Sync time (average)   =    0.00 sec.\n",
            "  Wait time (average)   =    0.00 sec.\n",
            "                          ------------\n",
            "Total (root+branch&cut) =    0.08 sec. (0.41 ticks)\n",
            "Solve status: JobSolveStatus.OPTIMAL_SOLUTION\n",
            "* model docplex_model4 solved with objective = -1428964620.800\n",
            "*  KPI: total gdp of julies over all selections  = 11697285.800\n",
            "*  KPI: total sick of julies over all selections = 533499.700\n",
            "model solve done in 0.1127786636352539 secs\n",
            "solution export done in 0.03911089897155762 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGwB7U9BB4Sf",
        "outputId": "0928615c-3f9f-46ba-9193-d7285641b1bb"
      },
      "source": [
        "print(outputs['SelectedJulies'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              july  selected julies decision  july sick  july gdp\n",
            "0          Atlanta                       1.0      46902    397261\n",
            "1        Baltimore                       1.0       7030    205313\n",
            "2           Boston                       1.0       4824    463570\n",
            "3        Charlotte                       1.0      12387    169862\n",
            "4          Chicago                       1.0      19759    689464\n",
            "5       Cincinnati                       1.0       6020    141042\n",
            "6        Cleveland                       1.0       5552    134369\n",
            "7         Columbus                       1.0       5752    129328\n",
            "8           Dallas                       1.0      60840    512509\n",
            "9           Denver                       1.0       5336    214157\n",
            "10         Detroit                       1.0       6543    267731\n",
            "11        Hartford                       1.0        782     99465\n",
            "12        Honolulu                       1.0        414     69252\n",
            "13         Houston                       1.0      56767    478778\n",
            "14    Indianapolis                       1.0       4712    140762\n",
            "15     Kansas City                       1.0       5844    132703\n",
            "16     Los Angeles                       1.0      74585   1047661\n",
            "17      Louisville                       1.0       2897     72093\n",
            "18         Memphis                       1.0       8839     76749\n",
            "19       Milwaukee                       1.0       5159    103731\n",
            "20     Minneapolis                       1.0       9172    263690\n",
            "21        New York                       1.0      13355   1772319\n",
            "22   Oklahoma City                       1.0       5553     81016\n",
            "23    Philadelphia                       1.0       9810    444148\n",
            "24      Pittsburgh                       1.0       3726    152840\n",
            "25        Portland                       1.0       4761    164419\n",
            "26      Providence                       1.0       1445     87414\n",
            "27         Raleigh                       1.0       6533     83665\n",
            "28        Richmond                       1.0       3172     85792\n",
            "29      Sacramento                       1.0      13341    145479\n",
            "30  Salt Lake City                       1.0       6545     94306\n",
            "31       San Diego                       1.0      18842    245138\n",
            "32   San Francisco                       1.0      26707    548613\n",
            "33        San Jose                       1.0      11235    331020\n",
            "34         Seattle                       1.0      10300    392036\n",
            "35        St Louis                       1.0       7591    169839\n",
            "36        Stamford                       1.0        612     89387\n",
            "37  Virginia Beach                       1.0       4343    100976\n",
            "38   Washington DC                       1.0       1369    540684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-8Iw9vOHe1Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}